{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8276fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class HardSigmoid(nn.Module):\n",
    "\n",
    "  def __init__(self, alpha=1.0):\n",
    "    super().__init__()\n",
    "    self.alpha = alpha\n",
    "\n",
    "  def forward(self, x):\n",
    "    return torch.clamp(torch.relu(self.alpha * x + 0.5), 0.0, 1.0)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = HardSigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=4):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        self.sigmoid = HardSigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class cbam_block(nn.Module):\n",
    "    def __init__(self, channel, ratio=4, kernel_size=3):\n",
    "        super(cbam_block, self).__init__()\n",
    "        self.channelattention = ChannelAttention(channel, ratio=ratio)\n",
    "        self.spatialattention = SpatialAttention(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.channelattention(x)\n",
    "        x = x * self.spatialattention(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def channel_shuffle(x, groups):\n",
    "    batch, channels, height, width = x.size()\n",
    "    channels_per_group = channels // groups\n",
    "    x = x.view(batch, groups, channels_per_group, height, width)\n",
    "    x = torch.transpose(x, 1, 2).contiguous()\n",
    "    x = x.view(batch, channels, height, width)\n",
    "    return x\n",
    "\n",
    "\n",
    "class ChannelShuffle(nn.Module):\n",
    "    def __init__(self, channels, groups):\n",
    "        super(ChannelShuffle, self).__init__()\n",
    "        if channels % groups != 0:\n",
    "            raise ValueError(\"The number of channels must be divisible by the number of groups.\")\n",
    "        self.groups = groups\n",
    "\n",
    "    def forward(self, x):\n",
    "        return channel_shuffle(x, self.groups)\n",
    "\n",
    "\n",
    "def Computing_mean(x, mask):\n",
    "    mask = torch.count_nonzero(mask, dim=2)\n",
    "    mask = torch.unsqueeze(mask, dim=2)\n",
    "    x = x.sum(dim=2, keepdim=True)\n",
    "    x = x / mask\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, F1: int, classes_num: int, D: int = 2):\n",
    "        super(CNN, self).__init__()\n",
    "        self.drop_out = 0.25\n",
    "        self.att = cbam_block(D * F1)\n",
    "\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.ZeroPad2d((7, 7, 0, 0)),\n",
    "            nn.Conv2d(1, F1, (1, 16), (1, 2), bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d((1, 8))\n",
    "        )\n",
    "\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.ZeroPad2d((7, 7, 0, 0)),\n",
    "            nn.Conv2d(F1, F1, (1, 16), (1, 2), bias=False, groups=F1),\n",
    "            nn.Conv2d(F1, D * F1, (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(D * F1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.Conv2d(D * F1, D * F1, (3, 1), groups=D * F1, bias=False),\n",
    "            nn.Conv2d(D * F1, D * D * F1, (1, 1), groups=4, bias=False),\n",
    "            nn.BatchNorm2d(D * D * F1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ChannelShuffle(D * D * F1, 4),\n",
    "        )\n",
    "\n",
    "        self.block_4 = nn.Sequential(\n",
    "            nn.ZeroPad2d((4, 3, 0, 0)),\n",
    "            nn.Conv2d(D * D * F1, D * D * F1, (1, 8), groups=D * D * F1, bias=False),\n",
    "            nn.BatchNorm2d(D * D * F1),\n",
    "            nn.Conv2d(D * D * F1, D * D * D * F1, (1, 1), groups=4, bias=False),\n",
    "            nn.BatchNorm2d(D * D * D * F1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d((1, 16))\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(self.drop_out),\n",
    "            nn.LazyLinear(classes_num),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) # (Batch, 1, Channels, time)\n",
    "\n",
    "        mask = torch.abs(x).sum(dim=3, keepdim=True) # (Batch, 1, Channels, 1) \n",
    "        mask = (mask > 0).type(torch.float)\n",
    "\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = x * mask\n",
    "\n",
    "        x1 = Computing_mean(x, mask)\n",
    "        x2 = torch.norm(x, p=2, dim=2, keepdim=True)\n",
    "        x3 = torch.norm(x, p=np.inf, dim=2, keepdim=True)\n",
    "        x = torch.cat([x1, x2, x3], 2)\n",
    "\n",
    "        x = self.att(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def forward_HW(self, x, n):\n",
    "      # n represents the number of dead channels\n",
    "      x = x.unsqueeze(1) # (Batch, 1, C, T)\n",
    "      if x.shape[0] > 1:\n",
    "        raise ValueError(\"Batch size must be 1 for HW inference\")\n",
    "      x = self.block_1(x)\n",
    "      x = self.block_2(x)\n",
    "\n",
    "      x = x[:, :, :n, :]\n",
    "\n",
    "      x1 = torch.mean(x, dim = 2, keepdim=True)\n",
    "      x2 = torch.norm(x, p=2, dim=2, keepdim=True)\n",
    "      x3 = torch.norm(x, p=np.inf, dim=2, keepdim=True)\n",
    "\n",
    "      x = torch.cat([x1, x2, x3], 2)\n",
    "      x = self.att(x)\n",
    "      x = self.block_3(x)\n",
    "      x = self.block_4(x)\n",
    "\n",
    "      x = x.view(x.shape[0], -1)\n",
    "      x = self.classifier(x)\n",
    "      return x\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = CNN(F1=16, classes_num=2, D=2).to(device)\n",
    "\n",
    "x = torch.randn(1, 64, 1000).to(device)\n",
    "pad = torch.zeros(1, 64, 1000).to(device)\n",
    "\n",
    "x = torch.cat([x, pad], dim=1) # x has now 64 padded channels\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    o_1 = model(x) # Normal training + inference for software\n",
    "    o_2 = model.forward_HW(x, 64) # Inference for hardware design (Batch has to be 1)\n",
    "\n",
    "\n",
    "# They should be equal\n",
    "\n",
    "print(o_1, o_2)\n",
    "print(o_1 == o_2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
