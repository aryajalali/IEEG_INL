{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BasicTransformer(nn.Module):\n",
    "    def __init__(self, args = None, hidden_dim = 16, num_layers = 2, nhead=1):\n",
    "        super(BasicTransformer, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "        if args is None:\n",
    "            args = lambda: None\n",
    "            args.segment_length = 500\n",
    "            args.sum_up_length = 5\n",
    "            args.segment_num = 7\n",
    "\n",
    "        c_N = (args.segment_length + 1) // 2 + (args.segment_length + 1) % 2\n",
    "        input_dim = c_N // args.sum_up_length\n",
    "\n",
    "        self.sum_up_length = args.sum_up_length\n",
    "\n",
    "        self.segment_num = args.segment_num\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 500, hidden_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=hidden_dim*4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers, enable_nested_tensor=False)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "\n",
    "\n",
    "    def differential_signed_min_max(self, x): # Creates the m x m matrix of differential segments and normalizes them\n",
    "        \"\"\"\n",
    "            b: batch_size\n",
    "            c: channel_num\n",
    "            m: segment_num\n",
    "            n: feature_dim\n",
    "        \"\"\"\n",
    "        b, c, m, n = x.shape\n",
    "        A = torch.unsqueeze(x, dim=-2)\n",
    "        B = torch.unsqueeze(x, dim=-3)\n",
    "        val = A - B # (b, c, m, m, n)\n",
    "        neg = val < 0\n",
    "        abs_c = torch.abs(val).view(b, c, -1, n)\n",
    "        max_val, _ = torch.max(abs_c, dim=-2)\n",
    "        max_val = torch.unsqueeze(max_val, dim=-2)\n",
    "        norm_val = abs_c / max_val\n",
    "        norm_val = norm_val.view(b, c, m, m, -1)\n",
    "        norm_val[neg] *= -1\n",
    "        norm_val = norm_val.permute(0, 1, 4, 2, 3)\n",
    "        return val.permute(0, 1, 4, 2, 3) # (b, c, n, m, m)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, p = None):\n",
    "\n",
    "        b, c, _ = x.shape\n",
    "\n",
    "        N = 2 * self.segment_num + 1\n",
    "\n",
    "        x = x.view(x.shape[0],c, N, -1)\n",
    "\n",
    "        x = x.view(x.shape[0],c, N, -1)\n",
    "        b = x.shape[0]\n",
    "\n",
    "        x = torch.log(torch.abs(torch.fft.rfft(x, dim=-1)) + 1)\n",
    "\n",
    "        c_N = x.shape[-1]\n",
    "        avai_c_N = c_N // self.sum_up_length * self.sum_up_length\n",
    "\n",
    "        x = x[:,:, :, :avai_c_N].view(b, c, N, -1, self.sum_up_length).sum(dim=-1)  # (b, 15, 50)\n",
    "\n",
    "\n",
    "        x = self.differential_signed_min_max(x) # (b, ic, N, N)\n",
    "\n",
    "\n",
    "        x = x.permute(0,1,3,4,2).contiguous().view(b,c, N * N, -1)\n",
    "\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        x = x.view(b * c, x.size(2), -1)\n",
    "\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "\n",
    "        encoded = self.transformer_encoder(x)\n",
    "\n",
    "        pooled = encoded.mean(dim=1)\n",
    "\n",
    "        out = self.fc_out(pooled)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "model = BasicTransformer()\n",
    "input_tensor = torch.randn(2, 3, 15 * 500) # Batch, Channel, Length = (2 * segment_num + 1) * segment_length\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(output.shape)  # Expected output shape: (6, 2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
